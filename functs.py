# -*- coding: utf-8 -*-
"""Image Classification with TF Hub
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/image_classification.ipynb
##### Copyright 2021 The TensorFlow Hub Authors.
Licensed under the Apache License, Version 2.0 (the "License");
"""

import time
import csv
##########
import tensorflow as tf
import tensorflow_hub as hub

import requests
from PIL import Image
from io import BytesIO

import matplotlib.pyplot as plt
import numpy as np
import sys
#####
import os
##########################
import pathlib
import matplotlib
import matplotlib.pyplot as plt
import io
import scipy.misc
from PIL import Image, ImageDraw, ImageFont
from six.moves.urllib.request import urlopen

#####

def load_image_into_numpy_array(path):
    image = None 
    if(path.startswith('http')):
        response = urlopen(path)
        image_data = response.read()
        image_data = BytesIO(image_data)
        image = Image.open(image_data)
    else:
        image_data = tf.io.gfile.GFile(path, 'rb').read()
        image = Image.open(BytesIO(image_data))
    (im_width, im_height) = image.size
    return np.array(image.getdata()).reshape((1, im_height, im_width, 3)).astype(np.uint8)


def preprocess_image(image):
  image = np.array(image)
  # reshape into shape [batch_size, height, width, num_channels]
  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)
  return image

def load_image_from_url(img_url):
  """Returns an image with shape [1, height, width, num_channels]."""
  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}
  response = requests.get(img_url, headers=user_agent)
  image = Image.open(BytesIO(response.content))
  image = preprocess_image(image)
  return image

def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):
  original_image_cache = {}
  """Loads and preprocesses images."""
  # Cache image file locally.
  if image_url in original_image_cache:
    img = original_image_cache[image_url]
  elif image_url.startswith('https://'):
    img = load_image_from_url(image_url)
  else:
    fd = tf.io.gfile.GFile(image_url, 'rb')
    img = preprocess_image(Image.open(fd))
  original_image_cache[image_url] = img
  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].
  img_raw = img
  if tf.reduce_max(img) > 1.0:
    img = img / 255.
  if len(img.shape) == 3:
    img = tf.stack([img, img, img], axis=-1)
  if not dynamic_size:
    img = tf.image.resize_with_pad(img, image_size, image_size)
  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:
    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)
  return img, img_raw

def show_image(image, title=''):
  image_size = image.shape[1]
  w = (image_size * 6) // 320
  fig=plt.figure(figsize=(w, w))
  plt.imshow(image[0], aspect='equal')
  plt.axis('off')
  plt.title(title)
  fig.savefig('image-classification.png', dpi=fig.dpi)
  plt.show()

"""Select an Image Classification Model. After that, some internal variables are set and the labels file is downloaded and prepared for use.
There are some technical differences between the models, like different input size, model size, accuracy, and inference time. Here you can change the model you are using until you find the one most suitable for your use case.
The handle (url) of the model is printed for your convenience. More documentation about each model is available there.
Note: All these models were trained on the ImageNet dataset
"""

#@title Select an Image Classification model



#model_name = "efficientnetv2-s" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']



def run_image_bench(self, model, lib, inter_list, intra_list, batch_size):
                
        if lib == "tp":
          os.environ['TF_ENABLE_ONEDNN_OPTS'] = "1"
        else:
          os.environ['TF_ENABLE_ONEDNN_OPTS'] = "0"
        

        model_name = model
        inter_op_threads = inter_list
        intra_op_threads = intra_list
        mb = batch_size
        benchname = lib
        print("*"*150)
        print("Model name =", model_name, " batch=", mb, " for ", benchname)
        # Set TF threads
        tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)
        tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)
        print("Inter threads = ", tf.config.threading.get_inter_op_parallelism_threads(), "AND Intra threads = ", tf.config.threading.get_intra_op_parallelism_threads())
        ############
        original_image_cache = {}

        # # # #

        image_size = 224
        dynamic_size = False

        model_handle_map = {
          "efficientnetv2-s": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2",
          "efficientnetv2-m": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2",
          "efficientnetv2-l": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2",
          "efficientnetv2-s-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2",
          "efficientnetv2-m-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2",
          "efficientnetv2-l-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2",
          "efficientnetv2-xl-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2",
          "efficientnetv2-b0-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2",
          "efficientnetv2-b1-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2",
          "efficientnetv2-b2-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2",
          "efficientnetv2-b3-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2",
          "efficientnetv2-s-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2",
          "efficientnetv2-m-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2",
          "efficientnetv2-l-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2",
          "efficientnetv2-xl-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2",
          "efficientnetv2-b0-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2",
          "efficientnetv2-b1-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2",
          "efficientnetv2-b2-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2",
          "efficientnetv2-b3-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2",
          "efficientnetv2-b0": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2",
          "efficientnetv2-b1": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2",
          "efficientnetv2-b2": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2",
          "efficientnetv2-b3": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2",
          "efficientnet_b0": "https://tfhub.dev/tensorflow/efficientnet/b0/classification/1",
          "efficientnet_b1": "https://tfhub.dev/tensorflow/efficientnet/b1/classification/1",
          "efficientnet_b2": "https://tfhub.dev/tensorflow/efficientnet/b2/classification/1",
          "efficientnet_b3": "https://tfhub.dev/tensorflow/efficientnet/b3/classification/1",
          "efficientnet_b4": "https://tfhub.dev/tensorflow/efficientnet/b4/classification/1",
          "efficientnet_b5": "https://tfhub.dev/tensorflow/efficientnet/b5/classification/1",
          "efficientnet_b6": "https://tfhub.dev/tensorflow/efficientnet/b6/classification/1",
          "efficientnet_b7": "https://tfhub.dev/tensorflow/efficientnet/b7/classification/1",
          "bit_s-r50x1": "https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1",
          "inception_v3": "https://tfhub.dev/google/imagenet/inception_v3/classification/4",
          "inception_resnet_v2": "https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4",
          "resnet_v1_50": "https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4",
          "resnet_v1_101": "https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4",
          "resnet_v1_152": "https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4",
          "resnet_v2_50": "https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4",
          "resnet_v2_101": "https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4",
          "resnet_v2_152": "https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4",
          "nasnet_large": "https://tfhub.dev/google/imagenet/nasnet_large/classification/4",
          "nasnet_mobile": "https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4",
          "pnasnet_large": "https://tfhub.dev/google/imagenet/pnasnet_large/classification/4",
          "mobilenet_v2_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4",
          "mobilenet_v2_130_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4",
          "mobilenet_v2_140_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4",
          "mobilenet_v3_small_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5",
          "mobilenet_v3_small_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5",
          "mobilenet_v3_large_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5",
          "mobilenet_v3_large_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5",
        }



        model_image_size_map = {
          "efficientnetv2-s": 384,
          "efficientnetv2-m": 480,
          "efficientnetv2-l": 480,
          "efficientnetv2-b0": 224,
          "efficientnetv2-b1": 240,
          "efficientnetv2-b2": 260,
          "efficientnetv2-b3": 300,
          "efficientnetv2-s-21k": 384,
          "efficientnetv2-m-21k": 480,
          "efficientnetv2-l-21k": 480,
          "efficientnetv2-xl-21k": 512,
          "efficientnetv2-b0-21k": 224,
          "efficientnetv2-b1-21k": 240,
          "efficientnetv2-b2-21k": 260,
          "efficientnetv2-b3-21k": 300,
          "efficientnetv2-s-21k-ft1k": 384,
          "efficientnetv2-m-21k-ft1k": 480,
          "efficientnetv2-l-21k-ft1k": 480,
          "efficientnetv2-xl-21k-ft1k": 512,
          "efficientnetv2-b0-21k-ft1k": 224,
          "efficientnetv2-b1-21k-ft1k": 240,
          "efficientnetv2-b2-21k-ft1k": 260,
          "efficientnetv2-b3-21k-ft1k": 300, 
          "efficientnet_b0": 224,
          "efficientnet_b1": 240,
          "efficientnet_b2": 260,
          "efficientnet_b3": 300,
          "efficientnet_b4": 380,
          "efficientnet_b5": 456,
          "efficientnet_b6": 528,
          "efficientnet_b7": 600,
          "inception_v3": 299,
          "inception_resnet_v2": 299,
          "mobilenet_v2_100_224": 224,
          "mobilenet_v2_130_224": 224,
          "mobilenet_v2_140_224": 224,
          "nasnet_large": 331,
          "nasnet_mobile": 224,
          "pnasnet_large": 331,
          "resnet_v1_50": 224,
          "resnet_v1_101": 224,
          "resnet_v1_152": 224,
          "resnet_v2_50": 224,
          "resnet_v2_101": 224,
          "resnet_v2_152": 224,
          "mobilenet_v3_small_100_224": 224,
          "mobilenet_v3_small_075_224": 224,
          "mobilenet_v3_large_100_224": 224,
          "mobilenet_v3_large_075_224": 224,
        }


        model_handle = model_handle_map[model_name]

        print(f"Selected model: {model_name} : {model_handle}")


        max_dynamic_size = 512
        if model_name in model_image_size_map:
          image_size = model_image_size_map[model_name]
          dynamic_size = False
          print(f"Images will be converted to {image_size}x{image_size}")
        else:
          dynamic_size = True
          print(f"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}")

        labels_file = "https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt"

        #download labels and creates a maps
        downloaded_file = tf.keras.utils.get_file("labels.txt", origin=labels_file)

        classes = []

        with open(downloaded_file) as f:
          labels = f.readlines()
          classes = [l.strip() for l in labels]

        """You can select one of the images below, or use your own image. Just remember that the input size for the models vary and some of them use a dynamic input size (enabling inference on the unscaled image). Given that, the method `load_image` will already rescale the image to the expected format."""

        #@title Select an Input Image

        image_name = "turtle" # @param ['tiger', 'bus', 'car', 'cat', 'dog', 'apple', 'banana', 'turtle', 'flamingo', 'piano', 'honeycomb', 'teapot']

        images_for_test_map = {
            "tiger": "https://upload.wikimedia.org/wikipedia/commons/b/b0/Bengal_tiger_%28Panthera_tigris_tigris%29_female_3_crop.jpg",
            #by Charles James Sharp, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons
            "bus": "https://upload.wikimedia.org/wikipedia/commons/6/63/LT_471_%28LTZ_1471%29_Arriva_London_New_Routemaster_%2819522859218%29.jpg",
            #by Martin49 from London, England, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons
            "car": "https://upload.wikimedia.org/wikipedia/commons/4/49/2013-2016_Toyota_Corolla_%28ZRE172R%29_SX_sedan_%282018-09-17%29_01.jpg",
            #by EurovisionNim, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons
            "cat": "https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg",
            #by Alvesgaspar, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons
            "dog": "https://upload.wikimedia.org/wikipedia/commons/archive/a/a9/20090914031557%21Saluki_dog_breed.jpg",
            #by Craig Pemberton, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons
            "apple": "https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg",
            #by Abhijit Tembhekar from Mumbai, India, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons
            "banana": "https://upload.wikimedia.org/wikipedia/commons/1/1c/Bananas_white_background.jpg",
            #by fir0002  flagstaffotos [at] gmail.com		Canon 20D + Tamron 28-75mm f/2.8, GFDL 1.2 <http://www.gnu.org/licenses/old-licenses/fdl-1.2.html>, via Wikimedia Commons
            "turtle": "https://upload.wikimedia.org/wikipedia/commons/8/80/Turtle_golfina_escobilla_oaxaca_mexico_claudio_giovenzana_2010.jpg",
            #by Claudio Giovenzana, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons
            "flamingo": "https://upload.wikimedia.org/wikipedia/commons/b/b8/James_Flamingos_MC.jpg",
            #by Christian Mehlführer, User:Chmehl, CC BY 3.0 <https://creativecommons.org/licenses/by/3.0>, via Wikimedia Commons
            "piano": "https://upload.wikimedia.org/wikipedia/commons/d/da/Steinway_%26_Sons_upright_piano%2C_model_K-132%2C_manufactured_at_Steinway%27s_factory_in_Hamburg%2C_Germany.png",
            #by "Photo: © Copyright Steinway & Sons", CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons
            "honeycomb": "https://upload.wikimedia.org/wikipedia/commons/f/f7/Honey_comb.jpg",
            #by Merdal, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons
            "teapot": "https://upload.wikimedia.org/wikipedia/commons/4/44/Black_tea_pot_cropped.jpg",
            #by Mendhak, CC BY-SA 2.0 <https://creativecommons.org/licenses/by-sa/2.0>, via Wikimedia Commons
        }

        img_url = images_for_test_map[image_name]
        image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)
        show_image(image, 'Scaled image')

        """Now that the model was chosen, loading it with TensorFlow Hub is simple.
        This also calls the model with a random input as a "warmup" run. Subsequent calls are often much faster, and you can compare this with the latency below.
        *Note:* models that use a dynamic size might need a fresh "warmup" run for each image size.
        """

        # Commented out IPython magic to ensure Python compatibility.
        classifier = hub.load(model_handle)

        input_shape = image.shape
        print("SHAPE BEFORE", input_shape)
        input_shape_final = (mb, input_shape[1], input_shape[2], input_shape[3])
        print("SHAPE AFTER", input_shape_final)
        warmup_input = tf.random.uniform(input_shape_final, 0, 1.0)
        warmup_logits = classifier(warmup_input).numpy()

        """Everything is ready for inference. Here you can see the top 5 results from the model for the selected image."""

        # Commented out IPython magic to ensure Python compatibility.
        # Run model on image
        inference_times = []
        tries=5
        for _ in range(tries):
            start_time = time.time_ns()
            probabilities = tf.nn.softmax(classifier(warmup_input)).numpy()
            end_time = time.time_ns()
            inference_time = np.round((end_time - start_time) / 1e6, 2)
            inference_times.append(inference_time)
            print('DONE,DONE', flush=True)
        print(inference_times)
        perf=np.min(inference_times)
        print("Inference time:", perf)
        return perf

def run_object_detect_bench(self, model, lib, inter_list, intra_list, batch_size):

        if lib == "tp":
            os.environ['TF_ENABLE_ONEDNN_OPTS'] = "1"
        else:
            os.environ['TF_ENABLE_ONEDNN_OPTS'] = "0"

        model_display_name = model
        inter_op_threads = inter_list
        intra_op_threads = intra_list
        mb = batch_size
        benchname = lib

        print("*"*150)
        print("Model name =", model_display_name, " batch=", mb, " for ", benchname)
        # Set TF threads
        tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)
        tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)
        print("Inter threads = ", tf.config.threading.get_inter_op_parallelism_threads(), "AND Intra threads = ", tf.config.threading.get_intra_op_parallelism_threads())
        ############
        tf.get_logger().setLevel('ERROR')


        ALL_MODELS = {
        'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',
        'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',
        'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',
        'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',
        'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',
        'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',
        'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',
        'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',
        'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',
        'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',
        'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',
        'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',
        'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',
        'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',
        'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',
        'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',
        'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',
        'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',
        'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',
        'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',
        'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',
        'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',
        'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',
        'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',
        'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',
        'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',
        'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',
        'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',
        'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',
        'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',
        'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',
        'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',
        'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',
        'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',
        'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',
        'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',
        'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',
        'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'
        }


        IMAGES_FOR_TEST = {
          'Beach' : '/root/asv-test/models/research/object_detection/test_images/image2.jpg',
          'Dogs' : '/root/asv-test/models/research/object_detection/test_images/image1.jpg',
          # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg
          'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',
         # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg
          'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',
          # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg
          'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',
          # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg
          'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',
        }

        COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),
         (0, 2),
         (1, 3),
         (2, 4),
         (0, 5),
         (0, 6),
         (5, 7),
         (7, 9),
         (6, 8),
         (8, 10),
         (5, 6),
         (5, 11),
         (6, 12),
         (11, 12),
         (11, 13),
         (13, 15),
         (12, 14),
         (14, 16)]

        from object_detection.utils import label_map_util
        from object_detection.utils import visualization_utils as viz_utils
        from object_detection.utils import ops as utils_ops

        PATH_TO_LABELS = '/root/asv-test/models/research/object_detection/data/mscoco_label_map.pbtxt'
        category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

        model_handle = ALL_MODELS[model_display_name]

        print('Selected model:'+ model_display_name)
        print('Model Handle at TensorFlow Hub: {}'.format(model_handle))

        print('loading model...')
        hub_model = hub.load(model_handle)
        print('model loaded!')

        selected_image = 'Phones' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']
        flip_image_horizontally = False #@param {type:"boolean"}
        convert_image_to_grayscale = False #@param {type:"boolean"}

        image_path = IMAGES_FOR_TEST[selected_image]
        image_np = load_image_into_numpy_array(image_path)
        print(image_np.shape)
        # Flip horizontally
        if(flip_image_horizontally):
          image_np[0] = np.fliplr(image_np[0]).copy()

        # Convert image to grayscale
        if(convert_image_to_grayscale):
          image_np[0] = np.tile(
            np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)

        #plt.figure(figsize=(24,32))
        #plt.imshow(image_np[0])
        #plt.show()
        input_shape = image_np.shape 
        print("SHAPE")
        print(mb, input_shape[1], input_shape[2], input_shape[3])
        input_shape = (mb, input_shape[1], input_shape[2], input_shape[3])
        print("SHAPE AFTER")
        print(mb, input_shape[1], input_shape[2], input_shape[3])

        warmup_input = tf.random.uniform(input_shape, 0, 1.0)
        
        inference_times = []
        tries=5
        for _ in range(tries):
            start_time = time.time_ns()
            results = hub_model(image_np)
            end_time = time.time_ns()
            inference_time = np.round((end_time - start_time) / 1e6, 2)
            inference_times.append(inference_time)
            print('DONE,DONE', flush=True)
        perf=np.min(inference_times)
        print("Inference time:", perf)
        return perf
