{
    "benchmarks.TensorflowMLPerfSuite.track_QPS_mobilenet": {
        "code": "class TensorflowMLPerfSuite:\n    def track_QPS_mobilenet(self):\n        out = subprocess.run(['bash /root/asv-test/run-mobilenet.sh'], shell=True, capture_output=True)\n        sout = out.stdout.decode(sys.stdout.encoding).split('\\n')\n        serr = out.stderr.decode(sys.stdout.encoding).split('\\n')\n        for i, line in enumerate(sout):\n            if 'qps=' in line:\n                result = re.search('qps=(.*?),', line)\n                finresult = result.group(1)\n                return float(finresult)\n        return 1.0\n\n    def setup_cache(self):\n        pass",
        "name": "benchmarks.TensorflowMLPerfSuite.track_QPS_mobilenet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "benchmarks:8",
        "timeout": 1800,
        "type": "track",
        "unit": "qps",
        "version": "e36085212cfca0c99334f4c5d5900d8fc8eb2c47d25a484bdbd74410f7b3b045"
    },
    "benchmarks.TensorflowMLPerfSuite.track_QPS_resnet50": {
        "code": "class TensorflowMLPerfSuite:\n    def track_QPS_resnet50(self):\n        out = subprocess.run(['bash /root/asv-test/run-resnet50.sh'], shell=True, capture_output=True)\n        sout = out.stdout.decode(sys.stdout.encoding).split('\\n')\n        serr = out.stderr.decode(sys.stdout.encoding).split('\\n')\n        for i, line in enumerate(sout):\n            if 'qps=' in line:\n                result = re.search('qps=(.*?),', line)\n                finresult = result.group(1)\n                return float(finresult)\n        return 1.0\n\n    def setup_cache(self):\n        pass",
        "name": "benchmarks.TensorflowMLPerfSuite.track_QPS_resnet50",
        "param_names": [],
        "params": [],
        "setup_cache_key": "benchmarks:8",
        "timeout": 1800,
        "type": "track",
        "unit": "qps",
        "version": "38e208d190fe48ddddbb7b8f88310321d015d7af5c7c68d835f0653fa346979e"
    },
    "imagebench.TensorflowImageClassification.track_image_classification": {
        "code": "class TensorflowImageClassification:\n    def track_image_classification(self, model, lib, inter_list, intra_list, batch_size):\n        import sys\n        sys.path.append('/root/asv-test')\n        import tensorflow as tf\n        from functs import preprocess_image, load_image_from_url, load_image, show_image\n        import os\n    \n        import time\n        import csv\n        ##########\n        import tensorflow as tf\n        import tensorflow_hub as hub\n    \n        import requests\n        from PIL import Image\n        from io import BytesIO\n    \n        import matplotlib.pyplot as plt\n        import numpy as np\n        import sys\n    \n        if model == \"tp\":\n          os.environ['TF_ENABLE_ONEDNN_OPTS'] = \"1\"\n        else:\n          os.environ['TF_ENABLE_ONEDNN_OPTS'] = \"0\"\n    \n    \n        model_name = model\n        inter_op_threads = inter_list\n        intra_op_threads = intra_list\n        mb = batch_size\n        benchname = lib\n        print(\"*\"*150)\n        print(\"Model name =\", model_name, \" batch=\", mb, \" for \", benchname)\n        # Set TF threads\n        tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)\n        tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)\n        print(\"Inter threads = \", tf.config.threading.get_inter_op_parallelism_threads(), \"AND Intra threads = \", tf.config.threading.get_intra_op_parallelism_threads())\n        ############\n        original_image_cache = {}\n    \n        # # # #\n    \n        image_size = 224\n        dynamic_size = False\n    \n        model_handle_map = {\n          \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n          \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",\n          \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2\",\n          \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2\",\n          \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2\",\n          \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2\",\n          \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2\",\n          \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2\",\n          \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2\",\n          \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2\",\n          \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2\",\n          \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2\",\n          \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2\",\n          \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2\",\n          \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2\",\n          \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\",\n          \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\",\n          \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2\",\n          \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2\",\n          \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n          \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2\",\n          \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2\",\n          \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2\",\n          \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n          \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/classification/1\",\n          \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\",\n          \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/classification/1\",\n          \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/classification/1\",\n          \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/classification/1\",\n          \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/classification/1\",\n          \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/classification/1\",\n          \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1\",\n          \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/classification/4\",\n          \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4\",\n          \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4\",\n          \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4\",\n          \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4\",\n          \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4\",\n          \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\",\n          \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4\",\n          \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/classification/4\",\n          \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\",\n          \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/classification/4\",\n          \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\",\n          \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\",\n          \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4\",\n          \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\",\n          \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5\",\n          \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5\",\n          \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5\",\n        }\n    \n    \n    \n        model_image_size_map = {\n          \"efficientnetv2-s\": 384,\n          \"efficientnetv2-m\": 480,\n          \"efficientnetv2-l\": 480,\n          \"efficientnetv2-b0\": 224,\n          \"efficientnetv2-b1\": 240,\n          \"efficientnetv2-b2\": 260,\n          \"efficientnetv2-b3\": 300,\n          \"efficientnetv2-s-21k\": 384,\n          \"efficientnetv2-m-21k\": 480,\n          \"efficientnetv2-l-21k\": 480,\n          \"efficientnetv2-xl-21k\": 512,\n          \"efficientnetv2-b0-21k\": 224,\n          \"efficientnetv2-b1-21k\": 240,\n          \"efficientnetv2-b2-21k\": 260,\n          \"efficientnetv2-b3-21k\": 300,\n          \"efficientnetv2-s-21k-ft1k\": 384,\n          \"efficientnetv2-m-21k-ft1k\": 480,\n          \"efficientnetv2-l-21k-ft1k\": 480,\n          \"efficientnetv2-xl-21k-ft1k\": 512,\n          \"efficientnetv2-b0-21k-ft1k\": 224,\n          \"efficientnetv2-b1-21k-ft1k\": 240,\n          \"efficientnetv2-b2-21k-ft1k\": 260,\n          \"efficientnetv2-b3-21k-ft1k\": 300,\n          \"efficientnet_b0\": 224,\n          \"efficientnet_b1\": 240,\n          \"efficientnet_b2\": 260,\n          \"efficientnet_b3\": 300,\n          \"efficientnet_b4\": 380,\n          \"efficientnet_b5\": 456,\n          \"efficientnet_b6\": 528,\n          \"efficientnet_b7\": 600,\n          \"inception_v3\": 299,\n          \"inception_resnet_v2\": 299,\n          \"mobilenet_v2_100_224\": 224,\n          \"mobilenet_v2_130_224\": 224,\n          \"mobilenet_v2_140_224\": 224,\n          \"nasnet_large\": 331,\n          \"nasnet_mobile\": 224,\n          \"pnasnet_large\": 331,\n          \"resnet_v1_50\": 224,\n          \"resnet_v1_101\": 224,\n          \"resnet_v1_152\": 224,\n          \"resnet_v2_50\": 224,\n          \"resnet_v2_101\": 224,\n          \"resnet_v2_152\": 224,\n          \"mobilenet_v3_small_100_224\": 224,\n          \"mobilenet_v3_small_075_224\": 224,\n          \"mobilenet_v3_large_100_224\": 224,\n          \"mobilenet_v3_large_075_224\": 224,\n        }\n    \n    \n        model_handle = model_handle_map[model_name]\n    \n        print(f\"Selected model: {model_name} : {model_handle}\")\n    \n    \n        max_dynamic_size = 512\n        if model_name in model_image_size_map:\n          image_size = model_image_size_map[model_name]\n          dynamic_size = False\n          print(f\"Images will be converted to {image_size}x{image_size}\")\n        else:\n          dynamic_size = True\n          print(f\"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}\")\n    \n        labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n    \n        #download labels and creates a maps\n        downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n    \n        classes = []\n    \n        with open(downloaded_file) as f:\n          labels = f.readlines()\n          classes = [l.strip() for l in labels]\n    \n        \"\"\"You can select one of the images below, or use your own image. Just remember that the input size for the models vary and some of them use a dynamic input size (enabling inference on the unscaled image). Given that, the method `load_image` will already rescale the image to the expected format.\"\"\"\n    \n        #@title Select an Input Image\n    \n        image_name = \"turtle\" # @param ['tiger', 'bus', 'car', 'cat', 'dog', 'apple', 'banana', 'turtle', 'flamingo', 'piano', 'honeycomb', 'teapot']\n    \n        images_for_test_map = {\n            \"tiger\": \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Bengal_tiger_%28Panthera_tigris_tigris%29_female_3_crop.jpg\",\n            #by Charles James Sharp, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n            \"bus\": \"https://upload.wikimedia.org/wikipedia/commons/6/63/LT_471_%28LTZ_1471%29_Arriva_London_New_Routemaster_%2819522859218%29.jpg\",\n            #by Martin49 from London, England, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n            \"car\": \"https://upload.wikimedia.org/wikipedia/commons/4/49/2013-2016_Toyota_Corolla_%28ZRE172R%29_SX_sedan_%282018-09-17%29_01.jpg\",\n            #by EurovisionNim, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n            \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg\",\n            #by Alvesgaspar, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n            \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/archive/a/a9/20090914031557%21Saluki_dog_breed.jpg\",\n            #by Craig Pemberton, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n            \"apple\": \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\",\n            #by Abhijit Tembhekar from Mumbai, India, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n            \"banana\": \"https://upload.wikimedia.org/wikipedia/commons/1/1c/Bananas_white_background.jpg\",\n            #by fir0002  flagstaffotos [at] gmail.com\t\tCanon 20D + Tamron 28-75mm f/2.8, GFDL 1.2 <http://www.gnu.org/licenses/old-licenses/fdl-1.2.html>, via Wikimedia Commons\n            \"turtle\": \"https://upload.wikimedia.org/wikipedia/commons/8/80/Turtle_golfina_escobilla_oaxaca_mexico_claudio_giovenzana_2010.jpg\",\n            #by Claudio Giovenzana, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n            \"flamingo\": \"https://upload.wikimedia.org/wikipedia/commons/b/b8/James_Flamingos_MC.jpg\",\n            #by Christian Mehlf\u00fchrer, User:Chmehl, CC BY 3.0 <https://creativecommons.org/licenses/by/3.0>, via Wikimedia Commons\n            \"piano\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/Steinway_%26_Sons_upright_piano%2C_model_K-132%2C_manufactured_at_Steinway%27s_factory_in_Hamburg%2C_Germany.png\",\n            #by \"Photo: \u00a9 Copyright Steinway & Sons\", CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n            \"honeycomb\": \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Honey_comb.jpg\",\n            #by Merdal, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons\n            \"teapot\": \"https://upload.wikimedia.org/wikipedia/commons/4/44/Black_tea_pot_cropped.jpg\",\n            #by Mendhak, CC BY-SA 2.0 <https://creativecommons.org/licenses/by-sa/2.0>, via Wikimedia Commons\n        }\n    \n        img_url = images_for_test_map[image_name]\n        image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)\n        show_image(image, 'Scaled image')\n    \n        \"\"\"Now that the model was chosen, loading it with TensorFlow Hub is simple.\n        This also calls the model with a random input as a \"warmup\" run. Subsequent calls are often much faster, and you can compare this with the latency below.\n        *Note:* models that use a dynamic size might need a fresh \"warmup\" run for each image size.\n        \"\"\"\n    \n        # Commented out IPython magic to ensure Python compatibility.\n        classifier = hub.load(model_handle)\n    \n        input_shape = image.shape\n        print(\"SHAPE BEFORE\", input_shape)\n        input_shape_final = (mb, input_shape[1], input_shape[2], input_shape[3])\n        print(\"SHAPE AFTER\", input_shape_final)\n        warmup_input = tf.random.uniform(input_shape_final, 0, 1.0)\n        warmup_logits = classifier(warmup_input).numpy()\n    \n        \"\"\"Everything is ready for inference. Here you can see the top 5 results from the model for the selected image.\"\"\"\n    \n        # Commented out IPython magic to ensure Python compatibility.\n        # Run model on image\n        inference_times = []\n        tries=5\n        for _ in range(tries):\n            start_time = time.time_ns()\n            probabilities = tf.nn.softmax(classifier(warmup_input)).numpy()\n            end_time = time.time_ns()\n            inference_time = np.round((end_time - start_time) / 1e6, 2)\n            inference_times.append(inference_time)\n            print('DONE,DONE', flush=True)\n        print(inference_times)\n        perf=np.min(inference_times)\n        print(\"Inference time:\", perf)\n        return perf\n        \"\"\"## Learn More\n        If you want to learn more and try how to do Transfer Learning with these models you can try this tutorial: [Transfer Learning for Image classification](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining)\n        If you want to check on more image models you can check them out on [tfhub.dev](https://tfhub.dev/s?module-type=image-augmentation,image-classification,image-classification-logits,image-classifier,image-feature-vector,image-generator,image-object-detection,image-others,image-pose-detection,image-segmentation,image-style-transfer,image-super-resolution,image-rnn-agent)\n        \"\"\"",
        "name": "imagebench.TensorflowImageClassification.track_image_classification",
        "param_names": [
            "param1",
            "param2",
            "param3",
            "param4",
            "param5"
        ],
        "params": [
            [
                "'inception_v3'",
                "'nasnet_mobile'"
            ],
            [
                "'tp'",
                "'eigen'"
            ],
            [
                "16"
            ],
            [
                "16"
            ],
            [
                "1",
                "16",
                "32"
            ]
        ],
        "timeout": 1800,
        "type": "track",
        "unit": "Inference Time",
        "version": "252f71a49d2f5abe376f717d91b19048a7f9fa798f747756461951374b044bac"
    },
    "version": 2
}